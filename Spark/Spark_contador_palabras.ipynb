{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa408b46e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#lista de palabras\n",
    "wordsList = ['spark', 'python', 'pypark', 'data','python']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print type(wordsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 2\n",
      "spark 1\n",
      "pypark 1\n",
      "data 1\n"
     ]
    }
   ],
   "source": [
    "result = wordsRDD.flatMap(lambda x: x.split(\" \")).countByValue()\n",
    "for key, value in result.iteritems():\n",
    "    print \"%s %i\" % (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Longitud de cada palabra **\n",
    "#### Usamos `map()` y la función  `lambda`  para devolver el número de caracteres de cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 6, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "wordsLengths = (wordsRDD.map(lambda x: len(x)).collect())\n",
    "print wordsLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Pares RDDs **\n",
    "#### El siguiente paso para escribir el contador de palabras es crear un nuevo tipo de RDD, llamado par RDD. Un par RDD es un RDD donde cada elemento es un par de tuplas `(k, v)` donde `k` es la clave y` v` es el valor. En este ejemplo, crearemos un par que consiste en `('<word>', 1)` para cada elemento de palabra en el RDD.\n",
    "#### Podemos crear el par RDD usando la transformación `map ()` con una función `lambda ()` para crear un nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark', 1), ('python', 1), ('pypark', 1), ('data', 1), ('python', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda x: (x, 1))\n",
    "print wordPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ahora, vamos a contar el número de veces que una palabra en particular aparece en el RDD. Hay varias maneras de realizar el conteo, pero algunas son más eficientes que otras.\n",
    "#### Un posible enfoque sería hacer un  `collect ()` de todos los elementos y contarlos en el programa de controlador. Si bien este enfoque podría funcionar para pequeños conjuntos de datos, queremos un enfoque que funcione para cualquier tamaño de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Usar `groupByKey()`  **\n",
    "#### Una posible solución es usar [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey). Como su nombre indica, la transformación `groupByKey ()` agrupa todos los elementos del RDD con la misma clave en una sola lista en una de las particiones. Hay dos problemas con el uso de `groupByKey ()`:\n",
    "   + #### La operación requiere una gran cantidad de movimiento de datos para mover todos los valores a las particiones apropiadas.\n",
    "   + #### No es óptimo para listas muy grandes.\n",
    " \n",
    "#### Use `groupByKey ()` para generar un par RDD de tipo `('word', contador)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: [1, 1]\n",
      "data: [1]\n",
      "spark: [1]\n",
      "pypark: [1]\n"
     ]
    }
   ],
   "source": [
    "wordsGrouped = wordPairs.groupByKey()\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print '{0}: {1}'.format(key, list(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### El uso de la función `groupByKey ()` crea un RDD que contiene 3 elementos, cada uno de los cuales es un par de una palabra y un iterador de Python.\n",
    "#### Ahora sume el iterador usando una transformación `map ()`. El resultado debe ser un par RDD que consiste en pares (palabra, contador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', 2), ('data', 1), ('spark', 1), ('pypark', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordCountsGrouped = wordsGrouped.map(lambda (k, v): (k, sum(v)))\n",
    "print wordCountsGrouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Usando `reduceByKey` **\n",
    "#### Una mejor solución es usar la función [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey). La transformación `reduceByKey ()` reúne pares que tienen la misma clave y aplica la función proporcionada a dos valores a la vez, reduciendo iterativamente todos los valores a un solo valor. `ReduceByKey ()` funciona aplicando la función primero dentro de cada partición en una base per-key y luego a través de las particiones, permitiéndole escalar eficientemente a conjuntos de datos grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', 2), ('data', 1), ('spark', 1), ('pypark', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordCounts = wordPairs.reduceByKey(lambda x, y: x + y)\n",
    "print wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Todo junto **\n",
    "#### La mejor versión es la que aplica la función `map()` sobre el RDD , aplica la transformación `reduceByKey()` , y al final llama a  `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', 2), ('data', 1), ('spark', 1), ('pypark', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda x: (x, 1))\n",
    "                       .reduceByKey(lambda x, y: x + y)\n",
    "                       .collect())\n",
    "print wordCountsCollected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** (3a) Palabras únicas **\n",
    "#### De forma sencilla podemos obtener el número de palabras únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = wordCounts.count()\n",
    "print uniqueWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Capitalización y puntuación **\n",
    "#### Al tratar con ficheros de texto es necesario tratar una serie de temas:\n",
    "   + #### Las palabras deben ser contadas independientemente si están en mayúsculas o minúsculas\n",
    "   + #### Eliminar signos de puntuación\n",
    "   + #### Eliminar los espacios iniciales o finales de una línea.\n",
    " \n",
    "#### Definimos la función `removePunctuation` que realiza estas tareas,convirtiendo todo el texto en minúsculas,eliminar signos de puntuación y espacios en blanco al principio y final de linea.  Para ello podemos usar el módulo de python  [re](https://docs.python.org/2/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return regex.sub('', text).lower().strip()\n",
    "print removePunctuation('Hi, you!')\n",
    "print removePunctuation('No under_score!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### También podemos definir una función que elimine aquello que no queremos contabilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removePunctuation(text):\n",
    "    return re.sub('[^a-z| |0-9]', '', text.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Cargar un fichero de texto **\n",
    "#### Para convertir un archivo de texto en un RDD, usamos el método `SparkContext.textFile ()`. También aplicamos la función `removePunctuation ()`  usando una transformación `map ()` para eliminar la puntuación y cambiar todo el texto a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "textRDD = (sc.textFile('about_spark.txt', 8).map(removePunctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Antes de poder usar la función `wordcount ()`, tenemos que abordar dos problemas con el formato del RDD:\n",
    "   + #### La primera cuestión es que necesitamos dividir cada línea por sus espacios.\n",
    "   + #### La segunda cuestión es que necesitamos filtrar líneas vacías.\n",
    " \n",
    "#### Aplicar una transformación que dividirá cada elemento del RDD por sus espacios. Para cada elemento del RDD, debe aplicar la función de cadena [split ()] (https://docs.python.org/2/library/string.html#string.split) de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textWordsRDD = textRDD.flatMap(lambda x: x.split())\n",
    "textWordCount = textWordsRDD.count()\n",
    "textWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'apache', u'spark', u'apache', u'spark', u'is', u'a', u'cluster', u'computing', u'platform', u'designed']\n"
     ]
    }
   ],
   "source": [
    "print textWordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ** Contar las palabras **\n",
    "#### A continuación vamos a detectar cual es la palabra que más veces aparece en el texto. Generaremos un ranking de las 10 más numerosas para que se vea parte del poder de spark.. Podemos ver las 10 palabras principales usando la acción `takeOrdered ()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 9),\n",
       " (u'and', 8),\n",
       " (u'spark', 8),\n",
       " (u'in', 8),\n",
       " (u'is', 7),\n",
       " (u'to', 6),\n",
       " (u'of', 4),\n",
       " (u'data', 4),\n",
       " (u'it', 4),\n",
       " (u'on', 3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking= textWordsRDD.map(lambda x: (x,1)).reduceByKey(lambda y,z: y+z).takeOrdered(10,lambda x : -x[1])\n",
    "ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
